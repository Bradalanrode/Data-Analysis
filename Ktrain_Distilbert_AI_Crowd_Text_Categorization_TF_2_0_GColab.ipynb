{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "Ktrain Distilbert AI Crowd Text Categorization TF 2.0.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14blNUWyfwVC",
        "colab_type": "text"
      },
      "source": [
        "## NLP Text Categorization Competition\n",
        "\n",
        "In this project I will be using a BERT model for classifying text data, as part of the AI Crowd NLP competition.\n",
        "\n",
        "I will be using the ktrain library, which works with Tensorflow 2.0. I found this high-level library to be extremely useful for quickly building up a great BERT model, with state of the art prediction capabilities. This project was done using a Google Colab notebook with GPU capabilities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WY4G53F7fwVG",
        "colab_type": "text"
      },
      "source": [
        "### The Contest: Transfer Learning for International Crisis Response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CalZvf1NfwVH",
        "colab_type": "text"
      },
      "source": [
        "The purpose of this contest is to improve the text classification algorithms used within the DEEP humanitarian information platform. To quote their website:\n",
        "\n",
        "\"The aim of the platform is to provide insights from years of historical and in-crisis humanitarian text data. The platform allows users to upload documents and classify text snippets according to predefined humanitarian target labels, grouped into and referred to as analytical frameworks. DEEP is now successfully functional in several international humanitarian organizations and the United Nations across the globe.\"\n",
        "\n",
        "Currently, multiple organizations use the DEEP platform and each has a separate classification algorithm with varying degrees of success and which are not trained across other organizations. The aim of this project is to build a unified algorithm which both performs better and is generalizable enough to work across all organizations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVc2i45YfwVI",
        "colab_type": "text"
      },
      "source": [
        "### The Categories: 12 Different Labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxUH3Xh_fwVJ",
        "colab_type": "text"
      },
      "source": [
        "(1) Agriculture\n",
        "\n",
        "(2) Cross: short form of Cross-sectoral; areas of humanitarian response that require action in more than one sector. For example malnutrition requires humanitarian interventions in health, access to food, access to basic hygiene items and clean water, and access to non-food items such as bottles to feed infants.\n",
        "\n",
        "(3) Education\n",
        "\n",
        "(4) Food\n",
        "\n",
        "(5) Health\n",
        "\n",
        "(6) Livelihood: Access to employment and income\n",
        "\n",
        "(7) Logistics: Any logistical support needed to carry out humanitarian activities e.g. air transport, satellite phone \n",
        "connection etc.\n",
        "\n",
        "(8) NFI: Non-food items needed in daily life that are not food such as bedding, mattrassess, jerrycans, coal or oil for heating\n",
        "\n",
        "(9) Nutrition\n",
        "\n",
        "(10) Protection\n",
        "\n",
        "(11) Shelter\n",
        "\n",
        "(12) WASH (Water, Sanitation and Hygiene)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_Hr6saDfwVK",
        "colab_type": "text"
      },
      "source": [
        "Although the contest has already finished while I am writing this, I never got the chance to try a BERT model. I wanted to build this notebook as a challenge to myself, to see if I can beat my previous best performance of 78% training accuracy and 69% validation accuracy. I am confident that BERT can help me achieve this!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpaaTcehfwVL",
        "colab_type": "text"
      },
      "source": [
        "Most of the data cleaning for this project was done in a different notebook then loaded into this one. To begin, I will import Tensorflow 2.0 into the notebook and verify that GPU hardware is attached with the code below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKTkccnQf2fl",
        "colab_type": "code",
        "outputId": "c75de788-a436-4633-f896-7147b31954d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDh-e0hZfwVM",
        "colab_type": "code",
        "outputId": "392ae969-b9e8-4b03-cb09-19d2da5d1b51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# installing the ktrain library we will be using to train our bert model\n",
        "\n",
        "! pip install ktrain\n",
        "\n",
        "# installing dependencies for ktrain\n",
        "\n",
        "! pip install git+https://github.com/amaiya/eli5@tfkeras_0_10_1\n",
        "! pip install git+https://github.com/amaiya/stellargraph@no_tf_dep_082"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ktrain in /usr/local/lib/python3.6/dist-packages (0.9.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from ktrain) (20.0)\n",
            "Requirement already satisfied: jieba in /usr/local/lib/python3.6/dist-packages (from ktrain) (0.40)\n",
            "Requirement already satisfied: requests in /tensorflow-2.1.0/python3.6 (from ktrain) (2.22.0)\n",
            "Requirement already satisfied: cchardet in /usr/local/lib/python3.6/dist-packages (from ktrain) (2.1.5)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.6/dist-packages (from ktrain) (1.3.2)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.6/dist-packages (from ktrain) (5.5.0)\n",
            "Requirement already satisfied: bokeh in /usr/local/lib/python3.6/dist-packages (from ktrain) (1.4.0)\n",
            "Requirement already satisfied: networkx==2.3 in /usr/local/lib/python3.6/dist-packages (from ktrain) (2.3)\n",
            "Requirement already satisfied: fastprogress>=0.1.21 in /usr/local/lib/python3.6/dist-packages (from ktrain) (0.2.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from ktrain) (0.14.1)\n",
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.6/dist-packages (from ktrain) (0.0.12)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (from ktrain) (2.3.0)\n",
            "Requirement already satisfied: keras-bert in /usr/local/lib/python3.6/dist-packages (from ktrain) (0.80.0)\n",
            "Requirement already satisfied: scikit-learn==0.21.3 in /usr/local/lib/python3.6/dist-packages (from ktrain) (0.21.3)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.6/dist-packages (from ktrain) (1.0.7)\n",
            "Requirement already satisfied: pandas<1.0 in /usr/local/lib/python3.6/dist-packages (from ktrain) (0.25.3)\n",
            "Requirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from ktrain) (3.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->ktrain) (2.4.6)\n",
            "Requirement already satisfied: six in /tensorflow-2.1.0/python3.6 (from packaging->ktrain) (1.14.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /tensorflow-2.1.0/python3.6 (from requests->ktrain) (2019.11.28)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /tensorflow-2.1.0/python3.6 (from requests->ktrain) (1.25.7)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /tensorflow-2.1.0/python3.6 (from requests->ktrain) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /tensorflow-2.1.0/python3.6 (from requests->ktrain) (3.0.4)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->ktrain) (2.3)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->ktrain) (0.15.2)\n",
            "Requirement already satisfied: termcolor in /tensorflow-2.1.0/python3.6 (from tensorflow-datasets->ktrain) (1.1.0)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->ktrain) (19.3.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /tensorflow-2.1.0/python3.6 (from tensorflow-datasets->ktrain) (3.11.2)\n",
            "Requirement already satisfied: absl-py in /tensorflow-2.1.0/python3.6 (from tensorflow-datasets->ktrain) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /tensorflow-2.1.0/python3.6 (from tensorflow-datasets->ktrain) (1.11.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->ktrain) (0.16.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->ktrain) (0.3.1.1)\n",
            "Requirement already satisfied: numpy in /tensorflow-2.1.0/python3.6 (from tensorflow-datasets->ktrain) (1.18.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->ktrain) (4.28.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython->ktrain) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython->ktrain) (1.0.18)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython->ktrain) (0.8.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /tensorflow-2.1.0/python3.6 (from ipython->ktrain) (45.0.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython->ktrain) (4.4.1)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython->ktrain) (4.7.0)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from ipython->ktrain) (4.3.3)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython->ktrain) (2.1.3)\n",
            "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.6/dist-packages (from bokeh->ktrain) (3.13)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from bokeh->ktrain) (2.6.1)\n",
            "Requirement already satisfied: pillow>=4.0 in /usr/local/lib/python3.6/dist-packages (from bokeh->ktrain) (6.2.2)\n",
            "Requirement already satisfied: Jinja2>=2.7 in /usr/local/lib/python3.6/dist-packages (from bokeh->ktrain) (2.10.3)\n",
            "Requirement already satisfied: tornado>=4.3 in /usr/local/lib/python3.6/dist-packages (from bokeh->ktrain) (4.5.3)\n",
            "Requirement already satisfied: Keras>=2.2.4 in /usr/local/lib/python3.6/dist-packages (from seqeval->ktrain) (2.2.5)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers->ktrain) (0.1.85)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers->ktrain) (2019.12.20)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers->ktrain) (1.10.47)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers->ktrain) (0.0.38)\n",
            "Requirement already satisfied: keras-transformer>=0.30.0 in /usr/local/lib/python3.6/dist-packages (from keras-bert->ktrain) (0.31.0)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /tensorflow-2.1.0/python3.6 (from scikit-learn==0.21.3->ktrain) (1.4.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas<1.0->ktrain) (2018.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0.0->ktrain) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0.0->ktrain) (0.10.0)\n",
            "Requirement already satisfied: googleapis-common-protos in /usr/local/lib/python3.6/dist-packages (from tensorflow-metadata->tensorflow-datasets->ktrain) (1.6.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->ktrain) (0.1.8)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython->ktrain) (0.6.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->ipython->ktrain) (0.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.7->bokeh->ktrain) (1.1.1)\n",
            "Requirement already satisfied: h5py in /tensorflow-2.1.0/python3.6 (from Keras>=2.2.4->seqeval->ktrain) (2.10.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /tensorflow-2.1.0/python3.6 (from Keras>=2.2.4->seqeval->ktrain) (1.0.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /tensorflow-2.1.0/python3.6 (from Keras>=2.2.4->seqeval->ktrain) (1.1.0)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.47 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers->ktrain) (1.13.47)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers->ktrain) (0.9.4)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers->ktrain) (0.2.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers->ktrain) (7.0)\n",
            "Requirement already satisfied: keras-pos-embd>=0.10.0 in /usr/local/lib/python3.6/dist-packages (from keras-transformer>=0.30.0->keras-bert->ktrain) (0.11.0)\n",
            "Requirement already satisfied: keras-embed-sim>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from keras-transformer>=0.30.0->keras-bert->ktrain) (0.7.0)\n",
            "Requirement already satisfied: keras-multi-head>=0.22.0 in /usr/local/lib/python3.6/dist-packages (from keras-transformer>=0.30.0->keras-bert->ktrain) (0.22.0)\n",
            "Requirement already satisfied: keras-layer-normalization>=0.12.0 in /usr/local/lib/python3.6/dist-packages (from keras-transformer>=0.30.0->keras-bert->ktrain) (0.14.0)\n",
            "Requirement already satisfied: keras-position-wise-feed-forward>=0.5.0 in /usr/local/lib/python3.6/dist-packages (from keras-transformer>=0.30.0->keras-bert->ktrain) (0.6.0)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.47->boto3->transformers->ktrain) (0.15.2)\n",
            "Requirement already satisfied: keras-self-attention==0.41.0 in /usr/local/lib/python3.6/dist-packages (from keras-multi-head>=0.22.0->keras-transformer>=0.30.0->keras-bert->ktrain) (0.41.0)\n",
            "Collecting git+https://github.com/amaiya/eli5@tfkeras_0_10_1\n",
            "  Cloning https://github.com/amaiya/eli5 (to revision tfkeras_0_10_1) to /tmp/pip-req-build-lzdhs9j1\n",
            "  Running command git clone -q https://github.com/amaiya/eli5 /tmp/pip-req-build-lzdhs9j1\n",
            "  Running command git checkout -b tfkeras_0_10_1 --track origin/tfkeras_0_10_1\n",
            "  Switched to a new branch 'tfkeras_0_10_1'\n",
            "  Branch 'tfkeras_0_10_1' set up to track remote branch 'tfkeras_0_10_1' from 'origin'.\n",
            "Requirement already satisfied (use --upgrade to upgrade): eli5==0.10.1 from git+https://github.com/amaiya/eli5@tfkeras_0_10_1 in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: attrs>16.0.0 in /usr/local/lib/python3.6/dist-packages (from eli5==0.10.1) (19.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from eli5==0.10.1) (2.10.3)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /tensorflow-2.1.0/python3.6 (from eli5==0.10.1) (1.18.1)\n",
            "Requirement already satisfied: scipy in /tensorflow-2.1.0/python3.6 (from eli5==0.10.1) (1.4.1)\n",
            "Requirement already satisfied: six in /tensorflow-2.1.0/python3.6 (from eli5==0.10.1) (1.14.0)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.6/dist-packages (from eli5==0.10.1) (0.21.3)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.6/dist-packages (from eli5==0.10.1) (0.10.1)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.6/dist-packages (from eli5==0.10.1) (0.8.6)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->eli5==0.10.1) (1.1.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.18->eli5==0.10.1) (0.14.1)\n",
            "Building wheels for collected packages: eli5\n",
            "  Building wheel for eli5 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for eli5: filename=eli5-0.10.1-py2.py3-none-any.whl size=106682 sha256=b5e5f9a3027f99d77871e3a7c7ac2e983a4bc27b45cc787bc7340d23af23728f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-gfwbms3w/wheels/51/59/0a/0f48442b8d209583a4453580938d7ba2270aca40edacee6d45\n",
            "Successfully built eli5\n",
            "Collecting git+https://github.com/amaiya/stellargraph@no_tf_dep_082\n",
            "  Cloning https://github.com/amaiya/stellargraph (to revision no_tf_dep_082) to /tmp/pip-req-build-2d1ms9sn\n",
            "  Running command git clone -q https://github.com/amaiya/stellargraph /tmp/pip-req-build-2d1ms9sn\n",
            "  Running command git checkout -b no_tf_dep_082 --track origin/no_tf_dep_082\n",
            "  Switched to a new branch 'no_tf_dep_082'\n",
            "  Branch 'no_tf_dep_082' set up to track remote branch 'no_tf_dep_082' from 'origin'.\n",
            "Requirement already satisfied (use --upgrade to upgrade): stellargraph==0.8.2 from git+https://github.com/amaiya/stellargraph@no_tf_dep_082 in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: numpy>=1.14 in /tensorflow-2.1.0/python3.6 (from stellargraph==0.8.2) (1.18.1)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /tensorflow-2.1.0/python3.6 (from stellargraph==0.8.2) (1.4.1)\n",
            "Requirement already satisfied: networkx<2.4,>=2.2 in /usr/local/lib/python3.6/dist-packages (from stellargraph==0.8.2) (2.3)\n",
            "Requirement already satisfied: scikit_learn>=0.20 in /usr/local/lib/python3.6/dist-packages (from stellargraph==0.8.2) (0.21.3)\n",
            "Requirement already satisfied: matplotlib>=2.2 in /usr/local/lib/python3.6/dist-packages (from stellargraph==0.8.2) (3.1.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.6/dist-packages (from stellargraph==0.8.2) (0.25.3)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx<2.4,>=2.2->stellargraph==0.8.2) (4.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit_learn>=0.20->stellargraph==0.8.2) (0.14.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2->stellargraph==0.8.2) (2.4.6)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2->stellargraph==0.8.2) (2.6.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2->stellargraph==0.8.2) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2->stellargraph==0.8.2) (1.1.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.24->stellargraph==0.8.2) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /tensorflow-2.1.0/python3.6 (from python-dateutil>=2.1->matplotlib>=2.2->stellargraph==0.8.2) (1.14.0)\n",
            "Requirement already satisfied: setuptools in /tensorflow-2.1.0/python3.6 (from kiwisolver>=1.0.1->matplotlib>=2.2->stellargraph==0.8.2) (45.0.0)\n",
            "Building wheels for collected packages: stellargraph\n",
            "  Building wheel for stellargraph (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for stellargraph: filename=stellargraph-0.8.2-cp36-none-any.whl size=146374 sha256=cd50238e46421e892114b0cdefc8917b8ac1c6d2f6487dbf2afe7b97358d2be6\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-5kc74cq2/wheels/29/6d/9d/505e95c414d36910e5c319ce9e63dc641973b2c6a2e2e16044\n",
            "Successfully built stellargraph\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8UqAfUEtfwVa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline\n",
        "import os\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\";\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\";"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9P-E38U1fwVd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# importing the ktrain library and pandas for preprocessing\n",
        "\n",
        "import ktrain\n",
        "from ktrain import text\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eog8gEqLfwVg",
        "colab_type": "code",
        "outputId": "2fac00d3-a56d-4228-b47a-3161da157d97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# loading data from csv file, this file was cleaned in another notebook\n",
        "# datasets will be separated into a train and test, with just the text column and the category it belongs to\n",
        "\n",
        "# load data\n",
        "data = pd.read_csv(\"train_no_dummies.csv\")\n",
        "\n",
        "# splitting and shuffling train and test, test size 10%\n",
        "train, test = train_test_split(data, test_size=0.1, shuffle=True, random_state=2)\n",
        "\n",
        "print('size of training set: %s' % (len(train['text'])))\n",
        "print('size of validation set: %s' % (len(test['text'])))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "size of training set: 8619\n",
            "size of validation set: 958\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LGV000ufwVj",
        "colab_type": "code",
        "outputId": "6a788f2f-cccc-4fe7-bc78-e6c4cb605595",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "# viewing the first 5 rows of training data\n",
        "\n",
        "train.head()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2521</th>\n",
              "      <td>The management of Mitiga Airport has confirmed...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4646</th>\n",
              "      <td>And in Venezuelaestán reappearing controladasc...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>825</th>\n",
              "      <td>Reportedly, transportation costs, accommodatio...</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5196</th>\n",
              "      <td>As for distance, 60% of households are less th...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8094</th>\n",
              "      <td>[Seasonal calendar] Climate conditions, Hazard...</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   text  labels\n",
              "2521  The management of Mitiga Airport has confirmed...       2\n",
              "4646  And in Venezuelaestán reappearing controladasc...       5\n",
              "825   Reportedly, transportation costs, accommodatio...      10\n",
              "5196  As for distance, 60% of households are less th...       2\n",
              "8094  [Seasonal calendar] Climate conditions, Hazard...       6"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8SGtB9RfwVm",
        "colab_type": "code",
        "outputId": "28cf6a10-6464-4bdb-c2f8-50b172df25c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "# splitting data into correct format for preprocessing\n",
        "x_train = train.text\n",
        "y_train = train.labels\n",
        "x_test = test.text\n",
        "y_test = test.labels\n",
        "\n",
        "# viewing some training examples before preprocessing\n",
        "x_train[:5]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2521    The management of Mitiga Airport has confirmed...\n",
              "4646    And in Venezuelaestán reappearing controladasc...\n",
              "825     Reportedly, transportation costs, accommodatio...\n",
              "5196    As for distance, 60% of households are less th...\n",
              "8094    [Seasonal calendar] Climate conditions, Hazard...\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRJFtyPXfwVq",
        "colab_type": "code",
        "outputId": "4b2d5191-3132-4883-a0b8-4f819739daa7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "text.print_text_classifiers()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fasttext: a fastText-like model [http://arxiv.org/pdf/1607.01759.pdf]\n",
            "logreg: logistic regression using a trainable Embedding layer\n",
            "nbsvm: NBSVM model [http://www.aclweb.org/anthology/P12-2018]\n",
            "bigru: Bidirectional GRU with pretrained word vectors [https://arxiv.org/abs/1712.09405]\n",
            "standard_gru: simple 2-layer GRU with randomly initialized embeddings\n",
            "bert: Bidirectional Encoder Representations from Transformers (BERT) [https://arxiv.org/abs/1810.04805]\n",
            "distilbert: distilled, smaller, and faster BERT from Hugging Face [https://arxiv.org/abs/1910.01108]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lLQdXQlfwVt",
        "colab_type": "code",
        "outputId": "869d7c84-f851-4ca1-d4bf-47f92ec94085",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "# we are using a distilbert model, a quicker training bert model that still works great\n",
        "\n",
        "# labels fall into one of 12 categories\n",
        "categories = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
        "\n",
        "# using ktrain's preprocessing function, I had to specify lang='en' (English) for the function to work\n",
        "trn, val, preproc = text.texts_from_array(x_train=x_train, y_train=y_train,\n",
        "                                          x_test=x_test, y_test=y_test,\n",
        "                                          class_names=categories,\n",
        "                                          preprocess_mode='distilbert',\n",
        "                                          lang='en',\n",
        "                                          maxlen=350)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "task: text classification\n",
            "preprocessing train...\n",
            "language: en\n",
            "train sequence lengths:\n",
            "\tmean : 63\n",
            "\t95percentile : 130\n",
            "\t99percentile : 185\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "preprocessing test...\n",
            "language: en\n",
            "test sequence lengths:\n",
            "\tmean : 63\n",
            "\t95percentile : 130\n",
            "\t99percentile : 181\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYHptAGMfwVv",
        "colab_type": "code",
        "outputId": "dcde982b-3a77-429c-ba73-e28494d30169",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "# defining model and wrapping in learner\n",
        "\n",
        "model = text.text_classifier('distilbert', train_data=trn, preproc=preproc)\n",
        "\n",
        "learner = ktrain.get_learner(model, train_data=trn, val_data=val, batch_size=6)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Is Multi-Label? False\n",
            "maxlen is 350\n",
            "done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEf19Nm3fwVy",
        "colab_type": "code",
        "outputId": "6a0ea149-969b-4d6a-a3f1-9b4084eed9b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "# viewing the layers of the model\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"tf_distil_bert_for_sequence_classification\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "distilbert (TFDistilBertMain multiple                  66362880  \n",
            "_________________________________________________________________\n",
            "pre_classifier (Dense)       multiple                  590592    \n",
            "_________________________________________________________________\n",
            "classifier (Dense)           multiple                  9997      \n",
            "_________________________________________________________________\n",
            "dropout_19 (Dropout)         multiple                  0         \n",
            "=================================================================\n",
            "Total params: 66,963,469\n",
            "Trainable params: 66,963,469\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOWlTZQofwV1",
        "colab_type": "code",
        "outputId": "80582da2-dc23-4e66-d897-2f3a9e2df168",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "# fitting model to data\n",
        "\n",
        "learner.fit_onecycle(3e-5, 1)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "begin training using onecycle policy with max lr of 3e-05...\n",
            "Train for 1437 steps, validate for 160 steps\n",
            "1437/1437 [==============================] - 385s 268ms/step - loss: 1.0444 - accuracy: 0.6908 - val_loss: 0.6949 - val_accuracy: 0.7871\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7feef605feb8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LL02l6refwV6",
        "colab_type": "text"
      },
      "source": [
        "That was a great first epoch! It is already much more accurate than my previous best model, with the validation accuracy at 78.7%. My previous best model only had a validation accuracy of 69%. Surprisingly, the training dataset's accuracy is currently lower than the val data after one epoch (69% on training and 79% on val). There is still much more room for improvement on training set accuracy with continued training. We will train for two more epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTOvzQtxfwV7",
        "colab_type": "code",
        "outputId": "e34d1ead-e150-4fca-fb06-e054dfdd9409",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "learner.fit_onecycle(3e-5, 2)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "begin training using onecycle policy with max lr of 3e-05...\n",
            "Train for 1437 steps, validate for 160 steps\n",
            "Epoch 1/2\n",
            "1437/1437 [==============================] - 377s 263ms/step - loss: 0.5371 - accuracy: 0.8442 - val_loss: 0.7133 - val_accuracy: 0.7933\n",
            "Epoch 2/2\n",
            "1437/1437 [==============================] - 376s 262ms/step - loss: 0.3999 - accuracy: 0.8879 - val_loss: 0.6665 - val_accuracy: 0.8038\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7feea8220710>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVcIP2RbfwWB",
        "colab_type": "text"
      },
      "source": [
        "## Final Thoughts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5d2TltpfwWB",
        "colab_type": "text"
      },
      "source": [
        "After two more epochs, accuracy on the training dataset improved dramatically, up to 89%, while the validation accuracy improved slightly, up to around 80.4%. There may be additional slight improvements on the validation data with more training, but at this point any further training may also run the risk of overfitting. I am sure there is much more fine-tuning that can be done with this project as well, including hyperparameter tuning and trying other BERT model variations, but for me this is a good stopping point. I am extremely happy with the improvements that the mighty BERT model helped me achieve."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppuHXwGVfwWC",
        "colab_type": "text"
      },
      "source": [
        "## Predictions On New Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2QYeWTxfwWD",
        "colab_type": "text"
      },
      "source": [
        "I will see how the model performs on sample text, a real example from the DEEP platform."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfjMO-gFfwWE",
        "colab_type": "code",
        "outputId": "3cf0fc0e-aab1-44fa-d508-4f9e4987d5f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "predictor = ktrain.get_predictor(model, preproc)\n",
        "\n",
        "predictor.predict(\"Data revealed that the majority (35 out of 46) of the restaurants and tea shops are cooking by using wood fuel in mud stove, while 16 of them are using LPG and LPG Stove\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhinja1qfwWH",
        "colab_type": "text"
      },
      "source": [
        "The model predicted the highest probability category of 9: Nutrition. This is a very reasonable label for a post about cooking methods, however if we were applying multiple labels, then 9: Food or also even 5: Health would also be good labels as well. In reality, the DEEP system does apply multiple labels, but they do not for purposes of this competition.\n",
        "\n",
        "Thank you BERT!"
      ]
    }
  ]
}